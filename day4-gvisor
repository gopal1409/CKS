##gvisor from google. 
  ##user sapce kernal for containers. 
  ##another layer of sepration
  ##not hypervisory or vm based as kata contianer
  ##it simulate kernal syscall with limited functionality
  ##runs in userspace sepratef from linux kernal 
  ##the run time is called as run sc
  ###lets create an use runtime classes

  ###runtime runsc gvisior
  ####run time is an specific class which help us to run my contianer in a runtime environment something called as virtualization
  ##specific pod will use the same runtime class. the node should suport runtime class
  ##run sc not yet installed on worker node
  ###
  https://kubernetes.io/docs/concepts/containers/runtime-class/
  ####
  root@k8smaster:~# vi rc.yml
  apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: gvisor
# The name of the corresponding CRI configuration
handler: runsc

root@k8smaster:~# kubectl -f rc.yml create
runtimeclass.node.k8s.io/gvisor created

root@k8smaster:~# kubectl get runtimeclass
NAME     HANDLER   AGE
gvisor   runsc     18s
##lets create a pod 
   kubectl run gvisor --image=nginx -o yaml --dry-run=client > gvisor.yml
####lets this pod use the runtime class
  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: gvisor
  name: gvisor
spec:
  runtimeClassName: gvisor ##add this classs
  containers:
  - image: nginx
    name: gvisor
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
~
###lets create thej pod 
  root@k8smaster:~# kubectl -f gvisor.yml create
pod/gvisor created
########ti will show in the hang state check the logs of the pod 
    root@k8smaster:~# kubectl describe pod gvisor
Name:                gvisor
Namespace:           default
Events:
  Type     Reason                  Age                From               Message
  ----     ------                  ----               ----               -------
  Normal   Scheduled               26s                default-scheduler  Successfully assigned default/gvisor to worker1
  Warning  FailedCreatePodSandBox  10s (x2 over 25s)  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to get sandbox runtime: no runtime for "runsc" 
####warning this installation can destroy your worker node
bash <(curl -s https://raw.githubusercontent.com/killer-sh/cks-course-environment/master/course-content/microservice-vulnerabilities/container-runtimes/gvisor/install_gvisor.sh)
###after the installtion check there is no error 
##check the status of containerd
  129  systemctl status containerd
 
  131  systemctl status kubelet
##then go back to master node and check how our pod is doing
##now you can see your gvisor pod is in running state
root@k8smaster:~# kubectl get pod
NAME        READY   STATUS    RESTARTS      AGE
accessor    1/1     Running   3 (58m ago)   3d18h
first-pod   1/1     Running   7 (58m ago)   8d
gvisor      1/1     Running   0             8m34s
##the pod is running in worker node
root@k8smaster:~# kubectl get pod -o wide
NAME        READY   STATUS    RESTARTS      AGE     IP            NODE      NOMINATED NODE   READINESS GATES
accessor    1/1     Running   3 (60m ago)   3d18h   192.168.1.3   worker1   <none>           <none>
first-pod   1/1     Running   7 (60m ago)   8d      192.168.1.2   worker1   <none>           <none>
gvisor      1/1     Running   0             9m49s   192.168.1.6   worker1   <none>           <none>
###loging insisde the pod 
root@k8smaster:~# kubectl exec -it gvisor -- /bin/bash
root@gvisor:/# uname -r
4.4.0
##here we will see the kernal is different
root@k8smaster:~# kubectl get nodes -o wide
NAME        STATUS   ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
k8smaster   Ready    control-plane   8d    v1.30.3   10.0.0.4      <none>        Ubuntu 20.04.6 LTS   5.15.0-1070-azure   containerd://1.6.12
worker1     Ready    <none>          8d    v1.30.3   10.0.0.5      <none>        Ubuntu 20.04.6 LTS   5.15.0-1070-azure   containerd://1.6.12
#########previsisly it show the exct version of linux kernal
again login inside the container and check the kernal
root@k8smaster:~# kubectl exec -it gvisor -- /bin/bash

root@gvisor:/# dmesg
[    0.000000] Starting gVisor...
[    0.484371] Feeding the init monster...
[    0.778255] Daemonizing children...
[    1.021320] Creating cloned children...
[    1.189031] Singleplexing /dev/ptmx...
[    1.343506] Granting licence to kill(2)...
[    1.646475] Forking spaghetti code...
[    1.989289] Conjuring /dev/null black hole...
[    2.251023] Moving files to filing cabinet...
[    2.269203] Synthesizing system calls...
[    2.423499] Searching for needles in stacks...
[    2.723302] Ready!
root@gvisor:/#
##now our pod is running in sand box
